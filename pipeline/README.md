# ðŸ”„ **Training Pipeline â€” MLOps Titanic Survival Prediction**

The `pipeline/` directory contains the **orchestration layer** for the entire MLOps workflow.
It coordinates all modular components under a **single, reproducible training entry point**, enabling the complete data-to-model process to run seamlessly from start to finish.

## ðŸ§¾ What this stage includes

* âœ… **`training_pipeline.py`** â€” integrates all pipeline stages:

  * **Data Ingestion** â†’ extracts the Titanic dataset from PostgreSQL
  * **Data Processing** â†’ cleans, engineers, and stores features in Redis
  * **Model Training** â†’ retrieves features from Redis, trains and saves a Random Forest model
* âœ… **End-to-end automation** â€” runs the full lifecycle with one command
* âœ… **Traceable artefacts** â€” logs, data files, and trained models are versioned under `artifacts/`

## âš™ï¸ Run the full training pipeline

To execute all stages sequentially:

```bash
python pipeline/training_pipeline.py
```

**Typical log output:**

```
2025-10-16 14:52:14,105 - INFO - ðŸš€ Starting Data Ingestion Pipeline...
2025-10-16 14:52:15,301 - INFO - âœ… Data Ingestion Pipeline completed successfully.
2025-10-16 14:52:15,302 - INFO - ðŸš€ Starting Data Processing pipeline...
2025-10-16 14:52:15,611 - INFO - âœ… Data Processing pipeline completed successfully.
2025-10-16 14:52:15,612 - INFO - ðŸš€ Starting Model Training pipeline...
2025-10-16 14:52:18,244 - INFO - âœ… Test Accuracy: 0.8351
2025-10-16 14:52:18,247 - INFO - ðŸ“¦ Model saved at: artifacts/models/random_forest_model.pkl
2025-10-16 14:52:18,248 - INFO - ðŸ End of Model Training pipeline.
```

## ðŸ—‚ï¸ Updated Project Structure

```
mlops-titanic-survival-prediction/
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ random_forest_model.pkl
â”œâ”€â”€ config/
â”œâ”€â”€ src/
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ training_pipeline.py      # ðŸ”„ Full end-to-end workflow
â”œâ”€â”€ logs/
â””â”€â”€ README.md
```

## ðŸ”— How this stage fits the pipeline

This script unifies all previous components into a **single orchestrated workflow**:

**PostgreSQL âžœ Data Ingestion âžœ Feature Processing âžœ Redis Store âžœ Model Training âžœ Model Artefact**

By centralising execution in one file, the pipeline provides a reproducible, auditable foundation for **deployment**, **CI/CD automation**, and **future model retraining**.

> ðŸ’¡ Next: the upcoming **Model Registry and Inference** stages will build on this pipeline to serve and monitor trained models in production.