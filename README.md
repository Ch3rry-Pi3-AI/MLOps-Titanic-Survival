# ğŸ”„ **Training Pipeline â€” MLOps Titanic Survival Prediction**

This stage introduces a **single orchestration script** that runs the whole workflow end-to-end:
**Data Ingestion âœ Feature Processing âœ Feature Store (Redis) âœ Model Training âœ Model Artefact**.

Itâ€™s a lightweight but reproducible entry point that ties together everything you built in earlier stages.

## ğŸ§¾ What this stage includes

* âœ… `pipeline/training_pipeline.py` â€” coordinates ingestion, processing, and training
* âœ… Saves a trained model to `artifacts/models/random_forest_model.pkl`
* âœ… Uses Redis Feature Store populated in the previous stage

## âš™ï¸ Run the full training pipeline

```bash
python pipeline/training_pipeline.py
```

**Typical output (abridged):**

```
2025-10-16 14:52:14,105 - INFO - ğŸš€ Starting Data Ingestion Pipeline...
2025-10-16 14:52:15,301 - INFO - âœ… Data Ingestion Pipeline completed successfully.
2025-10-16 14:52:15,302 - INFO - ğŸš€ Starting Data Processing pipeline...
2025-10-16 14:52:15,611 - INFO - âœ… Data Processing pipeline completed successfully.
2025-10-16 14:52:15,612 - INFO - ğŸš€ Starting Model Training pipeline...
2025-10-16 14:52:18,244 - INFO - âœ… Test Accuracy: 0.8351
2025-10-16 14:52:18,247 - INFO - ğŸ“¦ Model saved at: artifacts/models/random_forest_model.pkl
2025-10-16 14:52:18,248 - INFO - ğŸ End of Model Training pipeline.
```

## ğŸ—‚ï¸ Updated Project Structure

```
mlops-titanic-survival-prediction/
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ random_forest_model.pkl
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ database_config.py
â”‚   â””â”€â”€ paths_config.py
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ titanic.ipynb
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ training_pipeline.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ custom_exception.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ feature_store.py
â”‚   â”œâ”€â”€ feature_processing.py
â”‚   â””â”€â”€ model_training.py
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ log_YYYY-MM-DD.log
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## ğŸ”— Where this fits

You can now reproduce the entire **dataâ†’featuresâ†’model** flow with one command.
This sets the foundation for **CI/CD**, **scheduled retraining**, and downstream serving.

## ğŸš€ Next stage â€” Flask Inference App

Next, weâ€™ll build a **Flask app** to serve predictions to users, loading the saved model from `artifacts/models/` andâ€”optionallyâ€”pulling features from Redis on demand.