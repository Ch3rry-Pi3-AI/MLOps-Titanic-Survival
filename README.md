# ðŸ§  **Exploratory Analysis â€” MLOps Titanic Survival Prediction**

This branch represents the **data scientistâ€™s experimental stage**, where the **Titanic dataset** (previously ingested from a PostgreSQL database) is explored, cleaned, and analysed within a **Jupyter Notebook** environment.

The goal of this stage is to **understand passenger survival patterns**, perform **EDA and preprocessing experiments**, and develop an **early classification prototype** â€” before the workflow is modularised and automated by an ML engineer in the next stage.



## ðŸ§¾ **What This Stage Includes**

* âœ… Jupyter Notebook (`notebook/titanic.ipynb`) for interactive data exploration and experimentation
* âœ… Data ingestion from raw sources:

  * `titanic_train.csv` â€” passenger data extracted from PostgreSQL
* âœ… Initial data inspection (missing values, category distributions, duplicates)
* âœ… Preprocessing and encoding for categorical variables (`Sex`, `Embarked`)
* âœ… Feature engineering:

  * Family-based variables â€” `Familysize`, `Isalone`
  * Cabin indicator â€” `HasCabin`
  * Title extraction â€” `Mr`, `Mrs`, `Miss`, `Master`, `Rare`
  * Interaction features â€” `Pclass_Fare`, `Age_Fare`
* âœ… Handling of **class imbalance** via **SMOTE** oversampling
* âœ… Baseline **Random Forest Classifier** with **RandomizedSearchCV** for hyperparameter tuning
* âœ… Evaluation of model accuracy, feature importances, and class balance metrics

This notebook acts as a **sandbox for the data scientist** â€” a controlled environment to experiment freely before converting the logic into modular, production-ready scripts and pipeline stages.



## ðŸ—‚ï¸ **Updated Project Structure**

```
mlops-titanic-survival-prediction/
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ raw/                        # From previous Data Ingestion stage
â”‚   â”‚   â””â”€â”€ titanic_train.csv
â”‚   â””â”€â”€ processed/                  # Processed datasets, feature matrices, etc.
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ titanic.ipynb               # ðŸ” Data scientist EDA & experimentation
â”œâ”€â”€ config/
â”œâ”€â”€ src/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md                       # ðŸ“– You are here
```

> ðŸ’¡ The notebook uses data stored in `artifacts/raw/`, generated during the **Data Ingestion** stage.
> This dataset forms the foundation for feature engineering and model development in subsequent pipeline stages.



## ðŸ§© **Notebook Highlights**

Within `notebook/titanic.ipynb`, youâ€™ll find clearly structured sections covering:

1. **Setup & Imports** â€” loads dependencies, configures the working directory, and defines data paths.
2. **Data Loading & Quick Checks** â€” imports the Titanic dataset, previews it, and identifies missing values.
3. **Preprocessing & Encoding** â€” imputes missing data, encodes categorical variables, and standardises fields.
4. **Feature Engineering** â€” constructs derived features such as `Familysize`, `Isalone`, `HasCabin`, and `Title`.
5. **SMOTE Resampling** â€” balances survival classes to improve model generalisation.
6. **Train/Test Split** â€” separates data for unbiased model validation.
7. **Random Forest Modelling** â€” fits a classifier using Randomized Search for hyperparameter optimisation.
8. **Model Evaluation** â€” reports accuracy, classification metrics, and top feature importances.



## ðŸš€ **Next Stage â€” Data Processing**

In the next branch, this exploratory workflow evolves into the **Data Processing** stage â€” where the notebook logic is modularised into a reproducible preprocessing pipeline:

* Creation of `src/data_processing.py` for automated feature engineering, encoding, and cleaning.
* Update of `config/paths_config.py` to include processed data directories and file outputs.
* Structured artefacts saved under `artifacts/processed/` for downstream training and evaluation.
* Integration of robust logging and exception handling for traceability across environments.

This transition marks the evolution from **data science experimentation â†’ engineered preprocessing pipeline**, bridging the gap between **research and reproducible MLOps automation**.
