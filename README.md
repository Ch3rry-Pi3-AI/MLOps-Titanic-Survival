# üèóÔ∏è **Initial Project Setup ‚Äî MLOps Titanic Survival**

This branch establishes the **foundational structure** for the **MLOps Titanic Survival** project.
It sets up a **modular Python codebase** under `src/` and `pipeline/`, initialises dependency management with **`uv`**, and prepares the repository for the first workflow stage ‚Äî **`01_etl_pipeline`**, which will orchestrate an ETL process using **Astro Airflow** and **PostgreSQL**.



## üóÇÔ∏è **Project Structure**

```text
mlops-titanic-survival-prediction/
‚îú‚îÄ‚îÄ .venv/                     # üß© Local virtual environment (created by uv)
‚îú‚îÄ‚îÄ pipeline/                  # üîÑ Pipeline orchestration modules
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ src/                       # üß† Core source package (utility code, logging, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ .gitignore                 # üö´ Git ignore rules
‚îú‚îÄ‚îÄ pyproject.toml             # ‚öôÔ∏è Project metadata and uv configuration
‚îú‚îÄ‚îÄ requirements.txt           # üì¶ Python dependencies
‚îú‚îÄ‚îÄ uv.lock                    # üîí Dependency lock file (auto-generated by uv)
‚îî‚îÄ‚îÄ README.md                  # üìñ Project documentation (you are here)
```

> üí° **Note:** The `.venv/` directory is ignored by Git and should not be committed.



## ‚öôÔ∏è **Setup Process**

The following steps outline how this foundational setup was created.

### 1Ô∏è‚É£ Create the Project Structure

Directories were organised to separate **source code**, **pipeline logic**, and **configuration** for future stages.
Each directory includes an `__init__.py` file to make it importable as a Python package.

```bash
mkdir -p pipeline src
touch pipeline/__init__.py src/__init__.py
```



### 2Ô∏è‚É£ Create and Activate the Virtual Environment (with `uv`)

The project uses [`uv`](https://github.com/astral-sh/uv) ‚Äî a fast Python package manager ‚Äî instead of pip or Poetry.

Create and activate a Python 3.12 virtual environment:

```bash
uv venv --python 3.12
```

Activate it:

* **Windows (cmd):**

  ```cmd
  .\.venv\Scripts\activate
  ```

* **PowerShell:**

  ```powershell
  .\.venv\Scripts\Activate.ps1
  ```

* **macOS / Linux:**

  ```bash
  source .venv/bin/activate
  ```

Once activated, `( .venv )` will appear in your terminal prompt.



### 3Ô∏è‚É£ Define Project Dependencies

A minimal `requirements.txt` was created to support the base setup.
This list will expand as we introduce ETL, data validation, and orchestration functionality.

Example:

```text
pandas
numpy
sqlalchemy
psycopg2-binary
apache-airflow
astro-runtime
```

Install dependencies and generate a lock file:

```bash
uv pip install -r requirements.txt
uv lock
```



### 4Ô∏è‚É£ Define Project Metadata

A simple `pyproject.toml` ensures consistent environment and dependency handling.

Example:

```toml
[project]
name = "mlops-titanic-survival-prediction"
version = "0.1.0"
description = "An MLOps pipeline for the Titanic Survival Prediction project."
readme = "README.md"
requires-python = ">=3.12"

[tool.uv]
dev-dependencies = []
```

The `uv.lock` file pins exact dependency versions to guarantee reproducibility.



### 5Ô∏è‚É£ Add Core Source and Pipeline Packages

Both `src/` and `pipeline/` packages are initialised and ready to expand.
They will later include modules for logging, error handling, data ingestion, and orchestration.

| Folder      | Purpose                                                               |
| ----------- | --------------------------------------------------------------------- |
| `src/`      | Core utility code (e.g., logging, exceptions, shared helpers).        |
| `pipeline/` | Contains DAGs, operators, and transformation logic for ETL pipelines. |



## üöÄ **Next Steps**

In the next stage ‚Äî **`01_etl_pipeline`** ‚Äî the focus will be on:

* Ingesting the **Titanic dataset** from a **Google Cloud Storage** bucket.
* Loading and transforming the data into a **PostgreSQL** database using **Astro Airflow**.
* Implementing observability with detailed logging and idempotent task design.

This will serve as the foundation for future stages, including model training, evaluation, and serving.